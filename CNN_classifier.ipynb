{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import CIFAR10, ImageFolder\n",
    "import numpy as np\n",
    "from torch  import optim\n",
    "import torchvision.utils as vutil\n",
    "from tensorboard_logger import Logger\n",
    "import torchvision as tv\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from tensorboard_logger import configure, log_value, Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    lr=0.0002\n",
    "    nz=100# 噪声维度\n",
    "    image_size=64\n",
    "    image_size2=64\n",
    "    nc=3# 图片三通道\n",
    "    ngf=64 #生成图片\n",
    "    ndf=64 #判别图片\n",
    "    gpuids=2 #gpu\n",
    "    beta1=0.5\n",
    "    batch_size=256\n",
    "    max_epoch=12# =1 when debug\n",
    "    workers=4\n",
    "    \n",
    "opt=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyImageFolder(ImageFolder):\n",
    "    __init__ = ImageFolder.__init__\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return super(MyImageFolder, self).__getitem__(index)\n",
    "        except Exception as e:\n",
    "            if index>0:index=index -1\n",
    "            else: index = index+1\n",
    "            return self.__getitem__(index)\n",
    "\n",
    "# 训练数据集\n",
    "dataset= MyImageFolder('/home/x/data/pre/train_new/nouse2/',\n",
    "                transform=transforms.Compose([transforms.Scale(opt.image_size),\n",
    "                                              transforms.RandomCrop(opt.image_size) ,\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "                                             ]))\n",
    "dataloader=t.utils.data.DataLoader(dataset,opt.batch_size,True,num_workers=opt.workers)\n",
    "\n",
    "\n",
    "# 验证数据集\n",
    "val_dataset=MyImageFolder('/home/x/data/pre/test_new/nouse',\n",
    "                transform=transforms.Compose(\n",
    "                                             [transforms.Scale(opt.image_size),\n",
    "                                              transforms.RandomCrop(opt.image_size) ,\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "                                             ]))\n",
    "\n",
    "val_dataloader=t.utils.data.DataLoader(val_dataset, opt.batch_size, True, num_workers = opt.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model from GAN\n",
    "\n",
    "class ModelD(nn.Module):\n",
    "    def __init__(self,ngpu=None):\n",
    "        super(ModelD,self).__init__()\n",
    "        self.ngpu=ngpu\n",
    "        self.model=nn.Sequential()\n",
    "        self.model.add_module('conv1',nn.Conv2d(opt.nc,opt.ndf,4,2,1,bias=False))\n",
    "        self.model.add_module('relu1',nn.LeakyReLU(0.2,inplace=True))\n",
    "        \n",
    "        self.model.add_module('conv2',nn.Conv2d(opt.ndf,opt.ndf*2,4,2,1,bias=False))\n",
    "        self.model.add_module('bnorm2',nn.BatchNorm2d(opt.ndf*2))\n",
    "        self.model.add_module('relu2',nn.LeakyReLU(0.2,inplace=True))\n",
    "        \n",
    "        self.model.add_module('conv3',nn.Conv2d(opt.ndf*2,opt.ndf*4,4,2,1,bias=False))\n",
    "        self.model.add_module('bnorm3',nn.BatchNorm2d(opt.ndf*4))\n",
    "        self.model.add_module('relu3',nn.LeakyReLU(0.2,inplace=True))\n",
    "        \n",
    "        self.model.add_module('conv4',nn.Conv2d(opt.ndf*4,opt.ndf*8,4,2,1,bias=False))\n",
    "        self.model.add_module('bnorm4',nn.BatchNorm2d(opt.ndf*8))\n",
    "        self.model.add_module('relu4',nn.LeakyReLU(0.2,inplace=True))\n",
    "        \n",
    "        self.model.add_module('conv5',nn.Conv2d(opt.ndf*8,1,4,1,0,bias=False))\n",
    "#         self.model.add_module('sigmoid',nn.Sigmoid())\n",
    "    def forward(self,input):\n",
    "        gpuids=None\n",
    "        if self.ngpu:\n",
    "            gpuids=range(self.ngpu)\n",
    "        return nn.parallel.data_parallel(self.model,input, device_ids=gpuids)\n",
    "\n",
    "\n",
    "# define my classify model\n",
    "class FinetuneModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, ngpu=opt.gpuids):\n",
    "        self.ngpu = ngpu\n",
    "        super(FinetuneModel,self).__init__()\n",
    "        self.features = pretrained_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512 * 4 * 4, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 2))      \n",
    "        \n",
    "    def forward(self, x):\n",
    "        gpuids = None\n",
    "        if self.ngpu:\n",
    "            gpuids = range(self.ngpu)\n",
    "        features =  self.features(x).view(-1,512*4*4)#, x, device_ids=gpuids,ou)\n",
    "        return self.classifier(features)\n",
    "       # return nn.parallel.data_parallel(self.classifier, features, device_ids=gpuids)\n",
    "\n",
    "## tools to use for validate\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the precision@k for the specified values of k\n",
    "    @param output: score\n",
    "    @param target: label\n",
    "    \n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def val(model): \n",
    "    model.eval()\n",
    "    acc_meter = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    for ii,data in enumerate(val_dataloader):\n",
    "        input,label=data \n",
    "        val_input = Variable(input, volatile = True).cuda()\n",
    "        val_label = Variable(label.type(t.LongTensor), volatile = True).cuda()\n",
    "        score = model(val_input)\n",
    "        acc = accuracy(score.cpu().data,label)[0][0]\n",
    "        val_loss = t.nn.functional.cross_entropy(score,val_label)\n",
    "        n = input.size()[0]\n",
    "        acc_meter.update(acc,n)\n",
    "        loss_meter.update(val_loss.data,n)\n",
    "        if acc_meter.count>1000:break # val only 3000pictures one time\n",
    "    model.train()\n",
    "    return acc_meter, loss_meter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netd=ModelD(opt.gpuids)\n",
    "if hasattr(netd.model,'conv5'):del netd.model.conv5\n",
    "model = FinetuneModel(netd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(t.load('/home/cy/class_current_best_model_94.4813829787.nph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Linear\n",
      "a Linear\n",
      "a Linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FinetuneModel (\n",
       "  (features): ModelD (\n",
       "    (model): Sequential (\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (relu1): LeakyReLU (0.2, inplace)\n",
       "      (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu2): LeakyReLU (0.2, inplace)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu3): LeakyReLU (0.2, inplace)\n",
       "      (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu4): LeakyReLU (0.2, inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Dropout (p = 0.5)\n",
       "    (1): Linear (8192 -> 2048)\n",
       "    (2): ReLU (inplace)\n",
       "    (3): Dropout (p = 0.5)\n",
       "    (4): Linear (2048 -> 2048)\n",
       "    (5): ReLU (inplace)\n",
       "    (6): Linear (2048 -> 2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# def weight_init(m):\n",
    "#     #模型参数初始化．　可以优化成为xavier 初始化\n",
    "#     class_name=m.__class__.__name__\n",
    "#     if class_name.find('Conv')!=-1:\n",
    "        \n",
    "#         print 'Conv',class_name\n",
    "#         #xavier_normal(m.weight.data)\n",
    "#         print (m.weight.data.mean(),t.var(m.weight.data))\n",
    "#         m.weight.data.normal_(m.weight.data.mean(),t.var(m.weight.data)*10)\n",
    "# #         m.weight.data.add_(t.randn(*(m.weight.size())).cuda() * 0.005)\n",
    "# #         m.bias.data.zero_()\n",
    "#         m.weight.data.normal_(0,0)\n",
    "#     if class_name.find('Norm')!=-1:\n",
    "#         #xavier_normal(m.weight.data)\n",
    "#         print 'Norm',class_name\n",
    "#         print (m.weight.data.mean(),t.var(m.weight.data))\n",
    "#         m.weight.data.normal_(m.weight.data.mean(),t.var(m.weight.data)*10)\n",
    "#         m.bias.data.zero_()\n",
    "#     elif class_name.find('Linear')!=-1:\n",
    "#         xavier_normal(m.weight.data)\n",
    "#         m.bias.data.zero_()\n",
    "#         print 'a',class_name\n",
    "        #m.weight.data.normal_(0,0.02)\n",
    "def weight_init(m):\n",
    "    # 参数初始化。 可以改成xavier初始化方法\n",
    "    class_name=m.__class__.__name__\n",
    "    if class_name.find('Conv')!=-1: \n",
    "        m.weight.data.normal_(0,0.02)\n",
    "    if class_name.find('Norm')!=-1:\n",
    "        m.weight.data.normal_(1.0,0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif class_name.find('Linear')!=-1:\n",
    "        xavier_normal(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "        print 'a',class_name\n",
    "        #m.weight.data.normal_(0,0.02)\n",
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerFC=optim.Adam([{'params':model.features.parameters(),'lr':opt.lr},\n",
    "                        {'params':model.classifier.parameters(),'lr':opt.lr}\n",
    "                       ], betas = (opt.beta1, 0.999))\n",
    "\n",
    "input=Variable(t.FloatTensor(opt.batch_size,opt.nc,opt.image_size,opt.image_size2))#.cuda()\n",
    "label=Variable(t.LongTensor(opt.batch_size))#.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if opt.gpuids:\n",
    "    input = input.cuda()\n",
    "    label = label.cuda()\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:874: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    }
   ],
   "source": [
    "optimizerFC=optim.Adam([{'params':model.features.parameters(),'lr':0.1*opt.lr},\n",
    "                        {'params':model.classifier.parameters(),'lr':0.1*opt.lr}\n",
    "                       ], betas = (opt.beta1, 0.999))\n",
    "test_logger = Logger('../tensorboard_log/runs/cnn_classifier6')\n",
    "max_acc = 95\n",
    "for epoch in xrange(1,100):\n",
    "    for ii, data in enumerate(dataloader,0):\n",
    "        try:\n",
    "            model.zero_grad()\n",
    "            real,label_=data\n",
    "            input.data.resize_(real.size()).copy_(real)\n",
    "            label.data.resize_(input.size()[0]).copy_(label_)\n",
    "            output=model(input)\n",
    "            error_real=criterion(output,label)\n",
    "            error_real.backward()\n",
    "            D_x=output.data.mean()\n",
    "            optimizerFC.step()\n",
    "            \n",
    "            step += 1\n",
    "            if step>10:\n",
    "                test_logger.log_value('error_d',error_real.data[0],step)\n",
    "                test_logger.log_value('D_x',D_x,step)\n",
    "            if ii%10==0 and ii>0:\n",
    "                    a, l = val(model)\n",
    "                    if a.avg >max_acc: \n",
    "                        max_acc=a.avg\n",
    "                        t.save(model.state_dict(),'class_current_best_model_%s.nph' %max_acc)\n",
    "                    test_logger.log_value('val_loss',l.avg[0],step)\n",
    "                    test_logger.log_value('val_acc',a.avg,step)\n",
    "                    test_logger.log_value('epoch', epoch)\n",
    "                    test_logger.log_value('ii', ii)\n",
    "        except Exception as e:\n",
    "            print e\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data=iter(dataloader)\n",
    "optimizerFC=optim.Adam([{'params':model.features.parameters(),'lr':opt.lr},\n",
    "                        {'params':model.classifier.parameters(),'lr':opt.lr}\n",
    "                       ], betas = (opt.beta1, 0.999))\n",
    "if True:\n",
    "            model.zero_grad()\n",
    "            real,label_=data.next()\n",
    "            input.data.resize_(real.size()).copy_(real)\n",
    "            label.data.resize_(input.size()[0]).copy_(label_)\n",
    "            output=model(input)\n",
    "            error_real=criterion(output,label)\n",
    "            #error_real.backward()\n",
    "            D_x=output.data.mean()\n",
    "#             optimizerFC.step()\n",
    "            \n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', \n",
       "               1.0160\n",
       "               0.9839\n",
       "               1.0094\n",
       "               0.9934\n",
       "               0.9920\n",
       "               1.0041\n",
       "               0.9975\n",
       "               0.9501\n",
       "               1.0135\n",
       "               1.0041\n",
       "               0.9802\n",
       "               1.0213\n",
       "               1.0394\n",
       "               1.0022\n",
       "               0.9972\n",
       "               0.9704\n",
       "               0.9905\n",
       "               1.0094\n",
       "               0.9948\n",
       "               1.0138\n",
       "               0.9808\n",
       "               1.0118\n",
       "               0.9922\n",
       "               1.0142\n",
       "               0.9913\n",
       "               1.0277\n",
       "               0.9796\n",
       "               0.9981\n",
       "               1.0128\n",
       "               0.9911\n",
       "               0.9674\n",
       "               0.9841\n",
       "               1.0043\n",
       "               0.9747\n",
       "               1.0377\n",
       "               1.0220\n",
       "               1.0052\n",
       "               0.9788\n",
       "               1.0000\n",
       "               0.9728\n",
       "               0.9983\n",
       "               0.9861\n",
       "               0.9953\n",
       "               0.9728\n",
       "               1.0091\n",
       "               0.9724\n",
       "               1.0219\n",
       "               0.9822\n",
       "               1.0012\n",
       "               1.0421\n",
       "               1.0529\n",
       "               0.9778\n",
       "               0.9927\n",
       "               1.0208\n",
       "               1.0174\n",
       "               0.9832\n",
       "               1.0206\n",
       "               1.0112\n",
       "               1.0084\n",
       "               0.9893\n",
       "               0.9959\n",
       "               0.9965\n",
       "               1.0205\n",
       "               0.9863\n",
       "               0.9781\n",
       "               1.0205\n",
       "               1.0004\n",
       "               0.9554\n",
       "               1.0263\n",
       "               1.0021\n",
       "               1.0172\n",
       "               0.9799\n",
       "               1.0029\n",
       "               1.0008\n",
       "               1.0013\n",
       "               0.9974\n",
       "               0.9917\n",
       "               1.0181\n",
       "               0.9866\n",
       "               1.0097\n",
       "               1.0285\n",
       "               0.9950\n",
       "               0.9986\n",
       "               0.9860\n",
       "               0.9700\n",
       "               0.9898\n",
       "               1.0153\n",
       "               0.9983\n",
       "               0.9974\n",
       "               1.0048\n",
       "               0.9982\n",
       "               1.0000\n",
       "               0.9585\n",
       "               0.9989\n",
       "               1.0182\n",
       "               0.9992\n",
       "               0.9864\n",
       "               0.9887\n",
       "               1.0191\n",
       "               1.0082\n",
       "               0.9986\n",
       "               0.9799\n",
       "               1.0141\n",
       "               0.9948\n",
       "               0.9807\n",
       "               1.0104\n",
       "               0.9751\n",
       "               1.0335\n",
       "               1.0096\n",
       "               0.9821\n",
       "               0.9842\n",
       "               1.0104\n",
       "               0.9774\n",
       "               1.0274\n",
       "               0.9936\n",
       "               0.9753\n",
       "               1.0067\n",
       "               1.0255\n",
       "               1.0447\n",
       "               0.9876\n",
       "               0.9865\n",
       "               1.0349\n",
       "               0.9919\n",
       "               0.9694\n",
       "               0.9653\n",
       "               1.0257\n",
       "               1.0029\n",
       "               0.9917\n",
       "              [torch.FloatTensor of size 128]), ('bias', \n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "              [torch.FloatTensor of size 128]), ('running_mean', \n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "              [torch.FloatTensor of size 128]), ('running_var', \n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "               1\n",
       "              [torch.FloatTensor of size 128])])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm=model._modules.get('features')._modules['model']._modules['bnorm2']\n",
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "relu1 LeakyReLU (0.2, inplace)\n",
      "conv2 Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "bnorm2 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "relu2 LeakyReLU (0.2, inplace)\n",
      "conv3 Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "bnorm3 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "relu3 LeakyReLU (0.2, inplace)\n",
      "conv4 Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "bnorm4 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "relu4 LeakyReLU (0.2, inplace)\n"
     ]
    }
   ],
   "source": [
    "for (ii,jj) in  (model._modules.get('features')._modules['model']._modules).iteritems():print ii,jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = t.Tensor(128,128,16,16)\n",
    "def fn(x,y,z):tmp.copy_(z.data)\n",
    "r=model._modules.get('features')._modules['model']._modules['conv2'].register_forward_hook(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r={\n",
    "ii:mm.register_forward_hook(fn)\n",
    " for ii,mm in model._modules.get('features')._modules['model']._modules.iteritems()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -2.9443  1.2740 -1.3675 -3.5839\n",
      " -1.4194 -1.4231  1.3433  1.3199\n",
      "  1.1498  3.7366  2.0612 -4.7754\n",
      "  2.5386 -1.8528  0.9180  0.9751\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  0.4435  2.6998  0.4831  2.4340\n",
      " -0.8711  0.9448 -1.3786  4.0447\n",
      " -0.8067 -0.4061  1.3563  4.6852\n",
      " -3.5009  0.6244 -4.9069 -0.6132\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.2218  1.3931 -0.5838  1.0902\n",
      " -1.0052 -1.2321 -2.3953 -1.1548\n",
      "  2.6572  1.1532 -2.8928 -1.7039\n",
      "  0.4397  1.5250  2.4286  1.6358\n",
      "     ⋮ \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  0.6301 -0.4508 -2.5147  3.0496\n",
      " -1.3163  0.8824 -1.0645 -2.8138\n",
      "  1.0868  1.8104  1.0773  2.4678\n",
      "  1.2768 -0.9345 -2.3650  1.8713\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -0.4537  0.2801  1.9432 -0.9368\n",
      " -2.9854  0.7687  2.7918 -0.6559\n",
      " -2.3967 -0.2449  2.4610 -1.9675\n",
      " -1.6246  0.7765 -0.4811 -0.5398\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  2.6259  0.8802 -0.8576  1.7116\n",
      "  2.6498 -1.3863  0.3775  2.0457\n",
      " -1.7439  1.2663  1.2666  4.0195\n",
      " -0.4869 -0.8226  2.0717  3.8125\n",
      "     ⋮ \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -0.6513 -1.1051 -0.4527  0.5799\n",
      "  2.7754  5.2308  0.3958 -0.1329\n",
      "  1.1160 -1.5110 -3.0398 -0.8831\n",
      " -4.2462  2.8885 -1.3007  0.1315\n",
      "\n",
      "(2 ,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  0.9565  2.1236  0.2710  2.8189\n",
      " -4.4736 -0.1451 -0.4151  2.9557\n",
      "  2.7511 -1.6829 -1.0113  1.0102\n",
      "  2.1113 -4.2094  2.5183  1.8426\n",
      "\n",
      "(2 ,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.2691 -4.5523  1.1978 -0.6487\n",
      " -0.1970 -2.5765 -1.4341  1.2017\n",
      "  0.1817 -1.1769  1.6901 -1.0986\n",
      "  1.3342  2.2095 -0.1719 -1.1300\n",
      "...   \n",
      "     ⋮ \n",
      "\n",
      "(61,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  0.8887 -0.0607 -2.1171  3.9036\n",
      " -0.5569  1.5901 -1.8267 -4.5085\n",
      "  2.4829 -1.4637 -0.4854  2.2957\n",
      " -0.1156 -0.2624 -0.4000 -0.0591\n",
      "\n",
      "(61,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.8393 -2.1582 -1.5859  2.2272\n",
      "  0.3173 -4.3218 -0.4370 -3.6237\n",
      " -0.2193 -0.6607 -0.2388 -1.0894\n",
      "  0.2424  1.6666 -0.1922 -2.2397\n",
      "\n",
      "(61,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  0.6818  0.1400 -1.0665 -0.5609\n",
      "  1.3088  0.7017 -3.8199 -2.0043\n",
      " -1.0066  1.6206 -0.6507  0.8581\n",
      " -0.8919 -1.3894  5.0377 -0.4840\n",
      "     ⋮ \n",
      "\n",
      "(62,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -1.4351  1.4595 -1.3235 -0.1327\n",
      " -1.6361 -3.1121 -1.8046  0.2871\n",
      " -1.6738  0.4091 -0.4794 -0.0306\n",
      "  1.1593 -1.0332  0.3958 -0.7856\n",
      "\n",
      "(62,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  0.6503  1.8008  2.1194 -2.5884\n",
      " -1.1681  0.8098  1.5104  1.5056\n",
      " -1.9886 -3.0093 -3.7478 -2.8628\n",
      "  2.0675 -1.1764 -2.9483 -0.3335\n",
      "\n",
      "(62,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.0035 -0.3717  1.0901  0.1732\n",
      "  1.1255 -1.5298  1.5937  1.3196\n",
      "  1.9419  1.9103 -1.8653  1.2496\n",
      " -1.4796  2.2296  1.4660 -3.9719\n",
      "     ⋮ \n",
      "\n",
      "(63,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.9960  1.0574  0.7613  0.1532\n",
      " -1.5410  1.2339  1.2149  0.9747\n",
      "  0.4987  5.3999 -0.0742  1.0200\n",
      "  1.0183 -4.6548 -0.6205  0.1416\n",
      "\n",
      "(63,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  3.2884 -0.7319  1.2135 -1.0803\n",
      "  5.5728 -0.5075  1.1868  2.4940\n",
      "  0.0104 -2.1891  0.1714  2.3269\n",
      " -0.6881 -1.0281 -1.0585  0.2106\n",
      "\n",
      "(63,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  5.4639 -1.9666  2.0732  0.8287\n",
      " -2.6536  3.0367  1.1307  0.1087\n",
      "  1.1018  1.5025  1.9372 -3.2571\n",
      " -1.2859 -2.2637  0.3020  0.0711\n",
      "[torch.cuda.FloatTensor of size 64x3x4x4 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  4.6293e-02  2.1005e-02 -9.1097e-03  1.5091e-02\n",
      "  1.8214e-02  7.5812e-03  4.5620e-04  1.9891e-02\n",
      " -3.0026e-02  2.6004e-02 -3.5590e-03 -1.0893e-03\n",
      " -2.3617e-02  2.8861e-02  4.4037e-02  1.3623e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  2.7536e-02  1.3350e-02  2.0100e-02 -2.4593e-02\n",
      " -7.0389e-03 -9.2609e-03 -9.3264e-03 -2.8063e-03\n",
      " -1.6678e-03  1.0778e-02  4.2343e-03  1.9852e-03\n",
      " -1.8586e-02  2.5165e-02 -1.3161e-02 -8.8313e-03\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      " -9.3470e-03 -9.4835e-03  1.2004e-02  3.0651e-02\n",
      " -1.5430e-02  1.5575e-02 -2.0635e-02  2.5077e-02\n",
      "  1.2770e-02 -2.1556e-02  4.1311e-02 -7.4851e-03\n",
      " -4.7070e-03  2.5345e-02  2.0597e-02 -4.0561e-02\n",
      "    ... \n",
      "\n",
      "( 0 ,61 ,.,.) = \n",
      "  2.8654e-03 -7.0255e-04  1.0018e-02  3.4635e-02\n",
      "  1.0375e-02  1.7448e-02  3.6160e-03  2.9448e-03\n",
      " -1.0766e-02 -4.9835e-04 -3.0430e-02  1.4295e-02\n",
      "  5.8548e-04  5.8613e-03 -6.0737e-03 -4.4868e-02\n",
      "\n",
      "( 0 ,62 ,.,.) = \n",
      "  1.2614e-02 -5.1675e-02 -3.7904e-02 -3.0059e-02\n",
      "  1.2607e-02  2.8016e-02  3.0960e-03 -1.6485e-02\n",
      " -7.7752e-03  1.6748e-02  2.2457e-02 -5.8519e-03\n",
      "  3.4146e-02 -2.7281e-02  2.3868e-02 -2.4503e-02\n",
      "\n",
      "( 0 ,63 ,.,.) = \n",
      " -8.1840e-03  3.0056e-03  5.8426e-03  1.3482e-02\n",
      " -1.8177e-02 -6.8511e-03  1.9598e-03 -1.3044e-02\n",
      "  9.5567e-05  2.1594e-02 -2.1433e-02  1.8570e-02\n",
      " -1.5831e-02 -1.4767e-02 -7.1942e-03 -2.4031e-02\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      " -3.3266e-02  8.7895e-03 -1.4762e-02 -2.5989e-02\n",
      " -3.2835e-02  1.6795e-02  8.9487e-03  1.1452e-02\n",
      " -1.8567e-02  1.8450e-02 -1.0466e-02 -4.2573e-02\n",
      " -3.6368e-02  5.2355e-02 -1.7208e-02  1.7332e-04\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  4.1256e-04 -9.2308e-03 -2.0189e-02  5.5847e-03\n",
      " -3.3347e-03 -5.4177e-03 -1.0765e-02 -1.7407e-02\n",
      "  2.2935e-03 -1.0958e-02  3.5049e-02 -4.7032e-02\n",
      "  1.1668e-02  1.9397e-02  6.6747e-03 -4.5852e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -6.9095e-02  1.2769e-02 -2.7856e-02  1.0634e-02\n",
      "  5.9629e-03 -2.9312e-02 -2.6093e-02 -5.7050e-03\n",
      " -5.3934e-03  1.5786e-02  3.6195e-03  1.4936e-02\n",
      "  2.9675e-02  1.1445e-02 -1.3611e-03 -4.8262e-02\n",
      "    ... \n",
      "\n",
      "( 1 ,61 ,.,.) = \n",
      "  8.1262e-04 -1.4594e-02  3.5699e-02 -2.9436e-02\n",
      " -2.3343e-03  3.1307e-02 -9.2658e-03  8.6488e-03\n",
      "  2.6245e-02  1.0679e-02 -9.1315e-03  4.1120e-02\n",
      "  5.5499e-03  2.4470e-02  2.2511e-03  2.4065e-02\n",
      "\n",
      "( 1 ,62 ,.,.) = \n",
      " -3.1851e-02  3.0093e-02  4.7529e-03  2.9356e-02\n",
      "  6.6172e-04  2.5497e-02 -2.4339e-03  1.0349e-02\n",
      " -8.6680e-03 -5.0936e-02  1.9153e-02 -9.5466e-03\n",
      " -3.1292e-02 -1.8904e-02  8.4659e-03 -6.9451e-03\n",
      "\n",
      "( 1 ,63 ,.,.) = \n",
      " -4.4610e-02 -2.4379e-02 -2.4121e-02 -4.5122e-03\n",
      "  3.2897e-02 -7.0824e-03 -8.4041e-03 -1.2348e-02\n",
      "  1.1214e-02  7.1539e-03  2.3337e-02 -6.3695e-03\n",
      "  2.8923e-02 -2.6346e-02 -2.1651e-02  5.4519e-03\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  9.3287e-03 -2.6180e-03 -1.1384e-02  2.4535e-02\n",
      " -1.5856e-03  5.3356e-02 -2.7581e-02  2.5821e-04\n",
      "  2.0296e-03 -5.7953e-03 -7.6290e-03 -1.2421e-02\n",
      " -7.8074e-03  4.1989e-03  7.8418e-03  1.9510e-03\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      " -1.8353e-02  4.9552e-02  2.4331e-02  4.6407e-03\n",
      " -1.4553e-02 -1.8804e-02 -3.4822e-03 -4.1171e-03\n",
      " -2.0328e-02  2.8256e-02 -1.3305e-02 -2.8375e-02\n",
      "  2.8035e-03  2.6588e-02 -6.2523e-04  3.6498e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -1.2885e-02  5.1799e-03 -1.7698e-02  2.9887e-02\n",
      "  1.3377e-03 -2.3162e-02  6.4773e-03  4.0409e-02\n",
      "  3.0624e-02 -3.4001e-02 -3.9802e-02 -9.1291e-04\n",
      "  4.5183e-03  6.7624e-03 -4.5649e-02  2.8421e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,61 ,.,.) = \n",
      "  6.8663e-03 -1.7166e-02 -4.1187e-03 -1.4830e-02\n",
      " -2.2102e-03  6.7171e-03  2.1955e-02 -1.0018e-02\n",
      " -1.0581e-02  1.8556e-03 -1.0992e-02 -1.0031e-02\n",
      " -3.0694e-02  7.8556e-03 -1.8081e-03  1.0938e-02\n",
      "\n",
      "( 2 ,62 ,.,.) = \n",
      "  3.0110e-03  4.2641e-03 -7.4950e-04  1.7282e-02\n",
      "  2.6647e-03  3.1880e-02  2.0804e-03 -2.0488e-02\n",
      " -2.1704e-02  1.1026e-02 -3.9645e-02  1.1684e-02\n",
      "  1.0987e-02 -6.4587e-03  3.7772e-02 -1.0520e-03\n",
      "\n",
      "( 2 ,63 ,.,.) = \n",
      "  6.6549e-03 -9.2953e-03  2.6592e-02  3.0223e-02\n",
      " -2.4414e-02 -3.9766e-03  1.3954e-02 -1.1548e-02\n",
      " -2.0857e-02 -2.3680e-02 -5.9195e-03  1.1794e-02\n",
      " -1.9770e-03 -7.5939e-03 -6.8933e-03  2.1830e-02\n",
      "...     \n",
      "      ⋮  \n",
      "\n",
      "(125, 0 ,.,.) = \n",
      " -4.7695e-03 -1.4834e-02 -4.2772e-03  3.9314e-02\n",
      "  1.4343e-02  1.9941e-02 -1.5115e-02 -8.3648e-03\n",
      " -3.5880e-02  1.2542e-02  9.4754e-03  9.7772e-03\n",
      " -1.3083e-02 -8.0407e-03  1.2535e-02 -2.0943e-02\n",
      "\n",
      "(125, 1 ,.,.) = \n",
      "  1.7807e-02  3.0727e-02 -1.7004e-02  2.2677e-02\n",
      "  4.9439e-03 -3.3216e-02 -5.9386e-05 -3.1008e-02\n",
      " -2.4417e-02  1.4578e-02  2.2807e-02 -8.2856e-03\n",
      " -7.4835e-04 -1.2678e-02  5.9089e-03 -1.6706e-02\n",
      "\n",
      "(125, 2 ,.,.) = \n",
      " -2.9092e-03 -3.9820e-02  4.7575e-02 -2.6503e-02\n",
      "  1.9555e-02 -1.5738e-02  6.7871e-03  6.5481e-02\n",
      "  9.5242e-03  2.9696e-03 -2.5889e-02 -1.5061e-02\n",
      " -1.9551e-02  2.5022e-02  1.6972e-02 -3.1701e-02\n",
      "    ... \n",
      "\n",
      "(125,61 ,.,.) = \n",
      "  1.1471e-02 -6.5691e-04 -5.5615e-03  1.3189e-02\n",
      "  1.1676e-02  1.3413e-02 -2.3170e-03 -2.8349e-03\n",
      " -2.0325e-03  8.6833e-03  1.6215e-02  2.3082e-02\n",
      " -2.4450e-02 -2.3977e-02  2.0875e-02 -2.7246e-02\n",
      "\n",
      "(125,62 ,.,.) = \n",
      "  9.9317e-03 -1.8539e-03 -3.3590e-03 -4.0318e-03\n",
      "  2.7934e-02  1.2687e-02 -1.9732e-02  8.5600e-03\n",
      "  3.7481e-02  1.5141e-02  2.6960e-03  8.7068e-03\n",
      " -1.1609e-02 -4.7334e-02 -1.2450e-02  3.1988e-03\n",
      "\n",
      "(125,63 ,.,.) = \n",
      " -1.6655e-02  3.1665e-02 -2.9231e-03 -1.1107e-02\n",
      " -2.5980e-02  7.2547e-03 -3.0208e-02  1.1265e-02\n",
      "  2.5017e-02 -5.3462e-03 -2.4007e-02 -2.1385e-02\n",
      "  1.0798e-02 -1.8851e-02 -2.4371e-03  2.5764e-02\n",
      "      ⋮  \n",
      "\n",
      "(126, 0 ,.,.) = \n",
      " -8.6147e-03  1.4305e-02 -4.1063e-03 -1.4169e-02\n",
      " -2.2005e-05 -2.0925e-02  3.1291e-02  4.4816e-02\n",
      "  1.4809e-04  4.2264e-02 -1.8142e-02 -1.9907e-02\n",
      "  1.0998e-02 -1.6071e-02 -9.2756e-03  1.0477e-02\n",
      "\n",
      "(126, 1 ,.,.) = \n",
      " -1.4842e-02 -1.2451e-02 -3.6223e-02 -1.3080e-02\n",
      "  2.4081e-03  4.0778e-02  2.0918e-03 -2.1900e-02\n",
      " -5.7306e-03  3.7255e-02 -4.5609e-03  4.9461e-02\n",
      "  1.6918e-02 -2.6221e-02  3.0083e-02 -2.6566e-02\n",
      "\n",
      "(126, 2 ,.,.) = \n",
      "  5.6369e-03 -2.6388e-03  7.5358e-03  6.3592e-03\n",
      " -2.9029e-03 -3.6100e-02 -2.4836e-02  2.2290e-03\n",
      " -2.1511e-02 -9.0465e-03  1.3897e-02 -2.3426e-02\n",
      "  6.4130e-03  2.1204e-02  1.5144e-02  8.5494e-03\n",
      "    ... \n",
      "\n",
      "(126,61 ,.,.) = \n",
      " -1.4190e-02 -2.0076e-02  2.3420e-02  1.3370e-02\n",
      " -2.1060e-02  1.5943e-02 -1.1137e-02 -1.9513e-05\n",
      " -4.0173e-02  9.4793e-04  9.7729e-03  7.5826e-03\n",
      "  3.6542e-02 -3.1244e-02  4.3225e-02  6.8817e-03\n",
      "\n",
      "(126,62 ,.,.) = \n",
      " -1.4185e-02 -1.9180e-02 -3.8072e-02 -4.8014e-02\n",
      "  2.4282e-02 -6.8637e-03 -2.2681e-02 -2.2566e-03\n",
      " -3.6134e-05  1.7879e-02 -2.0761e-02  8.9290e-04\n",
      " -2.2658e-04 -2.7408e-02  3.9016e-02  5.6916e-03\n",
      "\n",
      "(126,63 ,.,.) = \n",
      " -3.8505e-02 -3.0793e-03  9.7924e-03  1.5180e-02\n",
      " -3.2040e-02  1.6526e-02 -2.3504e-02  4.2975e-05\n",
      " -1.3453e-02 -1.8477e-02 -1.7316e-03  1.7326e-02\n",
      "  1.0726e-02  3.4913e-02 -1.5088e-02 -1.4729e-02\n",
      "      ⋮  \n",
      "\n",
      "(127, 0 ,.,.) = \n",
      " -4.6632e-02  1.6606e-02 -3.8238e-02 -9.5892e-03\n",
      "  3.4535e-02  8.3115e-03 -2.3919e-02  3.8299e-02\n",
      " -2.7931e-02  1.6880e-02  2.0890e-02 -2.8144e-02\n",
      "  1.0697e-02  5.5182e-03 -5.1036e-02  4.5414e-03\n",
      "\n",
      "(127, 1 ,.,.) = \n",
      " -3.9700e-03 -1.3798e-02  2.4990e-02  6.9742e-03\n",
      "  6.6885e-04  2.5456e-02 -1.6228e-02  7.1621e-03\n",
      "  1.0419e-03 -2.1232e-02 -2.8912e-04 -2.5770e-02\n",
      " -2.3685e-02  1.5659e-02 -3.3384e-02 -2.0443e-02\n",
      "\n",
      "(127, 2 ,.,.) = \n",
      "  7.9844e-03 -1.4990e-02 -1.3147e-02  1.7000e-02\n",
      "  5.5510e-02  1.4536e-02  9.3162e-03  2.9617e-02\n",
      "  1.4228e-02  2.7097e-02  1.1374e-02  3.8754e-03\n",
      "  1.3148e-02 -1.1220e-02  1.7837e-02  9.7069e-03\n",
      "    ... \n",
      "\n",
      "(127,61 ,.,.) = \n",
      "  2.4332e-03 -1.6556e-02 -4.7116e-02 -7.8710e-03\n",
      " -3.3112e-02  1.7997e-02 -1.0744e-03 -2.7642e-04\n",
      " -3.1753e-02 -3.5304e-02 -1.7617e-02  5.3908e-03\n",
      "  2.5208e-02 -1.3699e-02  8.2154e-03 -3.6714e-02\n",
      "\n",
      "(127,62 ,.,.) = \n",
      " -4.3272e-02  2.8498e-02  4.1200e-02 -6.0927e-03\n",
      " -5.3261e-03 -2.0630e-02 -6.1137e-03  9.0044e-03\n",
      "  9.0570e-03  4.1710e-03  2.2852e-02  1.8456e-02\n",
      " -4.7023e-03  4.8892e-03 -1.8188e-03 -2.6751e-02\n",
      "\n",
      "(127,63 ,.,.) = \n",
      " -6.7515e-03  4.2240e-02 -1.9248e-02 -4.1603e-03\n",
      " -8.9108e-03  3.6474e-03 -3.9322e-02 -7.5708e-03\n",
      "  2.3966e-02  2.3363e-02  1.8347e-02 -3.9038e-02\n",
      "  2.4986e-02 -1.0242e-02 -1.7019e-02  4.8596e-02\n",
      "[torch.cuda.FloatTensor of size 128x64x4x4 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 1.0116\n",
      " 1.0103\n",
      " 1.0261\n",
      " 1.0188\n",
      " 1.0030\n",
      " 1.0272\n",
      " 1.0045\n",
      " 1.0113\n",
      " 1.0468\n",
      " 1.0065\n",
      " 1.0120\n",
      " 1.0227\n",
      " 1.0256\n",
      " 1.0049\n",
      " 0.9773\n",
      " 1.0045\n",
      " 1.0165\n",
      " 1.0153\n",
      " 0.9943\n",
      " 1.0054\n",
      " 0.9903\n",
      " 0.9961\n",
      " 1.0029\n",
      " 1.0181\n",
      " 1.0102\n",
      " 0.9919\n",
      " 1.0241\n",
      " 1.0282\n",
      " 1.0351\n",
      " 0.9941\n",
      " 1.0222\n",
      " 1.0032\n",
      " 0.9999\n",
      " 1.0166\n",
      " 1.0179\n",
      " 0.9954\n",
      " 1.0020\n",
      " 1.0060\n",
      " 0.9920\n",
      " 0.9797\n",
      " 0.9986\n",
      " 1.0179\n",
      " 0.9915\n",
      " 1.0130\n",
      " 0.9945\n",
      " 0.9834\n",
      " 1.0084\n",
      " 1.0217\n",
      " 0.9730\n",
      " 1.0062\n",
      " 0.9825\n",
      " 1.0071\n",
      " 1.0105\n",
      " 1.0082\n",
      " 1.0145\n",
      " 1.0206\n",
      " 1.0204\n",
      " 1.0131\n",
      " 1.0297\n",
      " 0.9788\n",
      " 0.9856\n",
      " 0.9827\n",
      " 0.9786\n",
      " 0.9805\n",
      " 0.9805\n",
      " 0.9762\n",
      " 1.0250\n",
      " 1.0184\n",
      " 0.9824\n",
      " 1.0384\n",
      " 1.0147\n",
      " 0.9857\n",
      " 0.9941\n",
      " 1.0025\n",
      " 0.9936\n",
      " 0.9789\n",
      " 0.9902\n",
      " 1.0474\n",
      " 1.0156\n",
      " 1.0206\n",
      " 1.0002\n",
      " 0.9734\n",
      " 1.0273\n",
      " 1.0072\n",
      " 1.0020\n",
      " 0.9829\n",
      " 1.0216\n",
      " 1.0329\n",
      " 0.9813\n",
      " 1.0163\n",
      " 0.9864\n",
      " 0.9925\n",
      " 1.0099\n",
      " 0.9920\n",
      " 1.0242\n",
      " 1.0103\n",
      " 0.9739\n",
      " 0.9937\n",
      " 1.0235\n",
      " 1.0219\n",
      " 1.0056\n",
      " 1.0065\n",
      " 1.0205\n",
      " 0.9798\n",
      " 0.9915\n",
      " 1.0262\n",
      " 0.9892\n",
      " 0.9921\n",
      " 0.9683\n",
      " 0.9995\n",
      " 1.0017\n",
      " 0.9895\n",
      " 0.9603\n",
      " 0.9837\n",
      " 0.9903\n",
      " 1.0096\n",
      " 0.9865\n",
      " 1.0126\n",
      " 1.0249\n",
      " 0.9982\n",
      " 1.0048\n",
      " 1.0204\n",
      " 0.9917\n",
      " 0.9826\n",
      " 1.0189\n",
      " 0.9835\n",
      " 0.9941\n",
      " 1.0196\n",
      "[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  2.2022e-02 -3.9516e-03 -3.2296e-02 -4.2439e-03\n",
      "  5.3159e-04  1.0111e-02  1.0483e-02 -1.8282e-03\n",
      " -1.1650e-03  2.3773e-02 -2.4195e-02  1.8608e-03\n",
      "  3.3957e-02  4.6173e-04 -1.9511e-02  3.9765e-03\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  1.3785e-02 -1.2606e-02 -1.1759e-02 -1.3422e-02\n",
      " -3.5385e-02  1.2839e-02  4.6550e-03  3.2136e-02\n",
      " -3.3581e-02 -3.3345e-03  3.0788e-02  1.1374e-02\n",
      "  8.5187e-03 -2.9376e-02 -2.2755e-02 -1.8777e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  3.4757e-02  1.4710e-02 -1.2002e-02  1.1717e-03\n",
      " -1.0295e-02 -5.6474e-02  2.2813e-02  3.0582e-02\n",
      "  1.6504e-02 -2.9779e-03  3.5171e-03  9.9863e-03\n",
      " -6.8214e-03  2.1813e-02  6.2975e-03 -5.6360e-03\n",
      "    ... \n",
      "\n",
      "( 0 ,125,.,.) = \n",
      "  2.8715e-02 -6.4657e-03  1.4790e-02  3.4257e-02\n",
      " -1.3967e-02 -7.5661e-04  3.1356e-03 -7.9326e-03\n",
      "  4.5471e-02 -9.3889e-03  2.5253e-02  5.2245e-03\n",
      " -8.0848e-04 -3.4865e-03 -1.9333e-02 -4.8599e-02\n",
      "\n",
      "( 0 ,126,.,.) = \n",
      " -9.2835e-03 -1.2291e-02  3.9323e-03 -2.8546e-02\n",
      " -2.2080e-03 -9.9016e-03 -1.4961e-03  2.3450e-02\n",
      " -2.7460e-03 -7.0443e-03 -1.9190e-02 -1.5087e-02\n",
      "  7.9269e-03 -8.2187e-03  5.0403e-02  7.5035e-04\n",
      "\n",
      "( 0 ,127,.,.) = \n",
      " -2.8525e-02  8.9885e-03  2.7963e-03  4.8672e-02\n",
      " -2.2491e-02 -7.5983e-03 -1.5710e-02  2.8284e-02\n",
      " -1.6748e-02  4.3529e-02  8.0797e-03 -2.0525e-02\n",
      " -2.3633e-02 -1.3741e-02 -1.3157e-03 -6.7262e-04\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  4.3950e-02 -3.2209e-02 -4.0475e-03  3.9609e-02\n",
      " -1.5270e-02  5.5760e-03  3.0700e-02 -2.2246e-03\n",
      "  1.2566e-02  3.0855e-03 -2.1127e-02 -1.0782e-02\n",
      " -1.9026e-02  2.2324e-02 -4.5962e-03 -1.5201e-02\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  1.4717e-02  3.9490e-02 -6.9690e-03  8.3139e-03\n",
      "  7.2247e-04  2.0302e-02 -1.0626e-02  1.3088e-03\n",
      " -1.3354e-02 -2.8346e-02 -5.0283e-02 -6.8865e-03\n",
      " -4.3173e-03  1.7980e-02 -1.2992e-02 -2.5743e-02\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      " -1.9906e-03  6.0552e-04  2.4701e-02  1.6020e-02\n",
      "  2.2116e-02 -7.1619e-03 -2.2102e-02 -2.7735e-02\n",
      "  1.2247e-02  3.4451e-02  5.2631e-03  1.8231e-02\n",
      " -1.8345e-03  5.5164e-03  2.3093e-02  1.5495e-02\n",
      "    ... \n",
      "\n",
      "( 1 ,125,.,.) = \n",
      " -1.9110e-02  3.4623e-03 -8.6947e-03  2.2045e-02\n",
      "  1.0863e-02  1.2427e-02 -2.9286e-02 -4.8661e-02\n",
      "  2.0223e-02  6.4558e-03  1.6683e-02 -1.1724e-02\n",
      "  1.7847e-02  1.9522e-02  2.2904e-02  1.3341e-02\n",
      "\n",
      "( 1 ,126,.,.) = \n",
      "  1.0231e-02  8.4383e-03  1.1370e-02  8.6430e-03\n",
      " -1.7123e-02  3.6219e-02 -3.3293e-02 -2.8174e-02\n",
      " -6.7407e-03 -1.1467e-02 -3.9277e-02 -5.8904e-03\n",
      " -3.3533e-02  3.3158e-03 -2.8739e-02 -2.1912e-02\n",
      "\n",
      "( 1 ,127,.,.) = \n",
      " -9.3188e-03  2.2938e-02  3.0875e-03  8.5403e-03\n",
      " -3.0355e-03 -1.6280e-02 -7.8426e-03  8.5293e-03\n",
      "  1.6263e-02 -4.6321e-02 -1.5185e-03 -5.2913e-03\n",
      " -5.3880e-03 -2.0046e-02 -1.6868e-02  1.6932e-02\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  9.0291e-03  3.8811e-03 -9.7447e-03  1.6273e-02\n",
      " -2.4864e-02 -9.5973e-03 -2.3200e-02  5.0104e-03\n",
      " -2.8788e-03 -1.5358e-02  7.9273e-03 -1.6996e-02\n",
      "  1.1169e-02  2.5513e-02 -3.7988e-02  4.5962e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      " -6.9989e-03  1.9564e-03 -1.5368e-03  2.0067e-02\n",
      "  4.0699e-03  2.4119e-02 -1.0766e-02 -1.8767e-02\n",
      "  5.4519e-02  2.9977e-02 -9.9622e-03  2.5024e-02\n",
      "  2.3826e-02 -1.8811e-02  2.4548e-02 -3.6961e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -1.5722e-02 -1.7081e-02  1.1903e-02 -2.2957e-03\n",
      " -3.5847e-03 -8.8494e-03 -1.9249e-02  3.1359e-02\n",
      "  2.1548e-02 -1.6548e-02 -1.0913e-02 -3.0821e-02\n",
      " -1.3775e-02 -7.4516e-03  1.0093e-02  2.0093e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,125,.,.) = \n",
      " -1.6449e-02  8.4992e-03 -1.6471e-02  1.4402e-02\n",
      " -4.1744e-03  1.1604e-02 -1.6245e-02 -1.2968e-02\n",
      " -3.3509e-02 -1.0817e-02 -1.4866e-02 -1.8594e-02\n",
      "  1.1021e-03 -3.9569e-03  2.9357e-02  4.5497e-03\n",
      "\n",
      "( 2 ,126,.,.) = \n",
      "  1.1632e-02  1.3520e-03  1.8501e-02  1.2078e-02\n",
      " -6.1131e-03 -1.4118e-02  4.5593e-02 -1.6757e-02\n",
      " -2.4610e-02  1.8764e-02 -4.5868e-03 -3.3478e-04\n",
      "  4.7401e-04 -6.1602e-04 -6.7599e-03  3.5265e-04\n",
      "\n",
      "( 2 ,127,.,.) = \n",
      "  2.1658e-02 -2.3297e-03 -2.5140e-02  1.3000e-02\n",
      " -1.6213e-02 -1.4374e-02  1.4333e-03  1.9034e-02\n",
      "  1.4135e-03 -1.5996e-02  4.2027e-02  1.7667e-02\n",
      "  5.2010e-03 -2.7790e-02  7.3089e-03 -2.6814e-02\n",
      "...     \n",
      "      ⋮  \n",
      "\n",
      "(253, 0 ,.,.) = \n",
      " -4.0845e-03 -2.6502e-02 -3.9957e-03  1.2666e-02\n",
      "  1.4891e-03  2.5166e-02 -2.7946e-02  3.7298e-02\n",
      "  1.6009e-02  2.7519e-02  4.0061e-02 -4.6128e-03\n",
      " -7.8763e-03 -1.9104e-03 -1.0687e-02 -1.5358e-02\n",
      "\n",
      "(253, 1 ,.,.) = \n",
      "  1.3284e-02  1.0835e-02 -2.5819e-02 -2.8011e-02\n",
      "  5.9975e-03 -1.1228e-02 -2.4979e-02 -2.0346e-02\n",
      "  2.8264e-02  1.7959e-02  2.4256e-02  4.1696e-03\n",
      " -7.9465e-03 -1.5447e-02 -7.2909e-03 -3.3998e-02\n",
      "\n",
      "(253, 2 ,.,.) = \n",
      " -7.2997e-03  3.0403e-02  1.4098e-02  2.8267e-02\n",
      "  3.6482e-04 -2.4714e-02  1.6877e-03 -2.7424e-03\n",
      "  1.8007e-02  1.6921e-02 -2.5298e-02  2.3328e-02\n",
      " -1.5787e-02 -4.3317e-02  2.1754e-03 -1.2691e-02\n",
      "    ... \n",
      "\n",
      "(253,125,.,.) = \n",
      " -2.6677e-02 -2.6477e-02 -2.1912e-02 -8.6252e-03\n",
      " -1.5742e-02  2.1144e-03  8.3532e-03 -6.7092e-03\n",
      " -2.2117e-02 -1.3739e-02 -3.5373e-03 -3.8484e-03\n",
      " -3.0076e-02  2.0026e-02  1.3582e-02  1.6873e-02\n",
      "\n",
      "(253,126,.,.) = \n",
      "  2.2188e-02 -1.9808e-03 -9.1055e-04  8.4465e-03\n",
      " -1.5266e-02  2.1651e-03 -3.9656e-03 -3.4710e-02\n",
      "  3.1560e-02  1.4716e-02  2.4335e-02 -1.8529e-02\n",
      " -2.9538e-02  3.2894e-02 -1.3650e-02  2.6502e-02\n",
      "\n",
      "(253,127,.,.) = \n",
      " -4.2582e-03  2.7876e-03 -1.5024e-02 -1.2605e-03\n",
      " -9.7747e-03  6.7331e-03  3.1830e-02 -2.5922e-03\n",
      " -1.4732e-03 -7.4858e-03 -2.1848e-02 -2.9364e-02\n",
      " -4.8102e-03 -2.6089e-02  1.8843e-02  1.5011e-02\n",
      "      ⋮  \n",
      "\n",
      "(254, 0 ,.,.) = \n",
      "  1.7276e-03  6.8816e-03 -1.8676e-02 -5.9752e-03\n",
      "  2.1145e-02  1.4021e-02 -4.1459e-02 -5.5024e-02\n",
      "  1.0125e-02 -1.3467e-02 -2.8858e-02 -6.8777e-03\n",
      "  2.7325e-02 -2.3882e-03 -3.8720e-02 -1.9856e-03\n",
      "\n",
      "(254, 1 ,.,.) = \n",
      "  1.0102e-02 -1.6020e-03 -2.8381e-03  2.1200e-02\n",
      " -8.5695e-03 -1.6923e-02 -5.1704e-03  1.4956e-02\n",
      " -1.3174e-02  1.2438e-03  3.1996e-02 -8.8936e-03\n",
      " -4.6356e-03 -2.2998e-02  2.6988e-02  8.9307e-03\n",
      "\n",
      "(254, 2 ,.,.) = \n",
      " -1.6160e-03  9.8263e-03 -4.7624e-03  3.8629e-03\n",
      " -1.0830e-02 -2.6766e-02 -1.0988e-03  1.3842e-02\n",
      "  4.2292e-02 -7.0345e-03 -1.1735e-02  3.2673e-02\n",
      "  1.0472e-02 -1.4330e-02 -8.1756e-03  3.1898e-02\n",
      "    ... \n",
      "\n",
      "(254,125,.,.) = \n",
      " -1.3426e-02  9.9199e-03  1.4075e-02 -1.1412e-02\n",
      "  3.0628e-02 -1.6601e-02 -1.7793e-02 -4.2661e-02\n",
      "  4.1186e-03 -8.6249e-03  3.7337e-02 -1.2734e-02\n",
      " -6.1113e-03 -2.2881e-02 -2.0071e-02 -1.1386e-02\n",
      "\n",
      "(254,126,.,.) = \n",
      "  2.6390e-03  5.1835e-03 -3.5433e-03  4.3749e-03\n",
      " -1.3562e-02 -1.4469e-02  3.3283e-03 -2.3759e-02\n",
      " -1.1934e-02  7.6076e-03 -1.9267e-02  2.0038e-02\n",
      " -1.2933e-02 -4.7790e-03  4.2443e-03 -9.3900e-03\n",
      "\n",
      "(254,127,.,.) = \n",
      "  2.9996e-03  9.7337e-03  1.1619e-02 -6.2305e-03\n",
      "  1.4713e-02 -1.6283e-02 -6.2125e-03  3.3663e-02\n",
      "  2.2151e-02 -1.0449e-02  4.4070e-02 -2.3890e-02\n",
      "  2.0458e-02  4.1644e-03  2.1067e-02 -4.7005e-04\n",
      "      ⋮  \n",
      "\n",
      "(255, 0 ,.,.) = \n",
      "  1.4925e-02  3.4345e-03  3.0545e-02 -1.0568e-02\n",
      " -3.1955e-03 -3.3575e-02  6.2591e-03  8.9999e-03\n",
      "  2.9614e-02  6.7451e-03  3.4673e-02 -8.4498e-03\n",
      "  4.7129e-02  8.1806e-03  2.6391e-02 -2.5011e-02\n",
      "\n",
      "(255, 1 ,.,.) = \n",
      "  9.8667e-03 -2.6393e-02 -1.6916e-02  4.6607e-03\n",
      " -1.9409e-02  1.5757e-02 -1.2499e-02 -2.0489e-02\n",
      "  2.2290e-02  3.6434e-02 -3.1450e-02  5.6816e-03\n",
      " -1.3848e-02 -2.6345e-02 -1.8398e-03 -1.9591e-02\n",
      "\n",
      "(255, 2 ,.,.) = \n",
      " -2.8190e-03  1.2082e-02 -3.2179e-02  1.0272e-02\n",
      "  1.2723e-02  6.6125e-03  7.6588e-03  1.4585e-02\n",
      " -5.0300e-03 -2.6771e-02  1.0991e-02 -5.8046e-03\n",
      " -1.8509e-02  8.9185e-04 -2.4302e-02  1.0459e-03\n",
      "    ... \n",
      "\n",
      "(255,125,.,.) = \n",
      "  2.0772e-03  2.1210e-02  1.6838e-02 -7.7917e-03\n",
      " -1.2311e-02 -2.9978e-02  4.1774e-02  1.6989e-02\n",
      "  3.2198e-02 -4.0766e-03 -1.0976e-02  1.2853e-02\n",
      "  9.0206e-05 -1.7651e-03  3.2866e-02  1.0265e-02\n",
      "\n",
      "(255,126,.,.) = \n",
      "  1.1324e-02 -6.4224e-02 -1.4386e-03 -2.4389e-03\n",
      "  2.4708e-02 -1.2581e-02 -1.8460e-02 -1.2273e-02\n",
      "  4.8661e-02 -2.3061e-02 -5.5555e-03  5.5929e-02\n",
      " -1.8820e-02  1.1556e-02 -1.9071e-02 -1.4681e-02\n",
      "\n",
      "(255,127,.,.) = \n",
      " -1.5883e-03 -7.7014e-03 -2.3380e-02  2.0969e-02\n",
      "  3.6582e-02 -2.7031e-02  7.8179e-04 -5.9445e-03\n",
      " -1.6776e-02 -1.4334e-02  1.7744e-02  1.7113e-02\n",
      " -8.3227e-03  3.9094e-03 -1.6142e-02  4.4118e-03\n",
      "[torch.cuda.FloatTensor of size 256x128x4x4 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 1.0105\n",
      " 0.9979\n",
      " 1.0127\n",
      " 0.9736\n",
      " 0.9833\n",
      " 0.9832\n",
      " 0.9922\n",
      " 0.9729\n",
      " 0.9935\n",
      " 1.0307\n",
      " 0.9935\n",
      " 1.0074\n",
      " 0.9980\n",
      " 0.9915\n",
      " 1.0290\n",
      " 1.0156\n",
      " 1.0108\n",
      " 0.9993\n",
      " 1.0008\n",
      " 1.0175\n",
      " 1.0259\n",
      " 1.0192\n",
      " 0.9828\n",
      " 0.9760\n",
      " 0.9788\n",
      " 0.9809\n",
      " 1.0009\n",
      " 1.0015\n",
      " 1.0144\n",
      " 0.9975\n",
      " 0.9953\n",
      " 1.0491\n",
      " 0.9836\n",
      " 0.9746\n",
      " 0.9693\n",
      " 0.9793\n",
      " 1.0233\n",
      " 0.9687\n",
      " 1.0116\n",
      " 0.9956\n",
      " 1.0013\n",
      " 1.0130\n",
      " 0.9973\n",
      " 1.0025\n",
      " 0.9872\n",
      " 0.9701\n",
      " 1.0071\n",
      " 1.0292\n",
      " 0.9893\n",
      " 0.9832\n",
      " 1.0041\n",
      " 0.9857\n",
      " 0.9808\n",
      " 1.0255\n",
      " 0.9800\n",
      " 0.9816\n",
      " 0.9866\n",
      " 0.9827\n",
      " 0.9711\n",
      " 1.0101\n",
      " 1.0098\n",
      " 1.0148\n",
      " 1.0070\n",
      " 0.9992\n",
      " 1.0200\n",
      " 0.9809\n",
      " 1.0301\n",
      " 1.0130\n",
      " 1.0009\n",
      " 0.9746\n",
      " 1.0227\n",
      " 0.9856\n",
      " 1.0071\n",
      " 1.0093\n",
      " 0.9692\n",
      " 1.0090\n",
      " 1.0270\n",
      " 0.9966\n",
      " 1.0147\n",
      " 1.0089\n",
      " 0.9695\n",
      " 1.0172\n",
      " 1.0022\n",
      " 0.9746\n",
      " 0.9966\n",
      " 1.0245\n",
      " 1.0001\n",
      " 1.0197\n",
      " 1.0272\n",
      " 1.0065\n",
      " 1.0060\n",
      " 1.0120\n",
      " 1.0097\n",
      " 0.9955\n",
      " 1.0284\n",
      " 0.9548\n",
      " 0.9652\n",
      " 1.0030\n",
      " 0.9895\n",
      " 1.0194\n",
      " 1.0229\n",
      " 1.0003\n",
      " 0.9927\n",
      " 1.0090\n",
      " 0.9994\n",
      " 0.9734\n",
      " 1.0194\n",
      " 0.9740\n",
      " 0.9724\n",
      " 1.0177\n",
      " 0.9818\n",
      " 0.9890\n",
      " 1.0206\n",
      " 0.9768\n",
      " 0.9830\n",
      " 0.9461\n",
      " 1.0036\n",
      " 1.0116\n",
      " 1.0112\n",
      " 0.9983\n",
      " 1.0109\n",
      " 1.0301\n",
      " 1.0208\n",
      " 0.9900\n",
      " 1.0015\n",
      " 1.0010\n",
      " 1.0110\n",
      " 1.0381\n",
      " 1.0008\n",
      " 0.9939\n",
      " 1.0233\n",
      " 0.9980\n",
      " 0.9940\n",
      " 1.0027\n",
      " 1.0220\n",
      " 1.0291\n",
      " 1.0125\n",
      " 1.0056\n",
      " 0.9934\n",
      " 1.0355\n",
      " 0.9915\n",
      " 0.9868\n",
      " 0.9870\n",
      " 1.0084\n",
      " 0.9729\n",
      " 1.0188\n",
      " 0.9917\n",
      " 0.9938\n",
      " 1.0055\n",
      " 0.9626\n",
      " 0.9958\n",
      " 1.0078\n",
      " 0.9913\n",
      " 1.0227\n",
      " 1.0172\n",
      " 1.0077\n",
      " 1.0196\n",
      " 0.9744\n",
      " 0.9803\n",
      " 1.0094\n",
      " 1.0219\n",
      " 0.9918\n",
      " 1.0040\n",
      " 0.9903\n",
      " 1.0143\n",
      " 0.9874\n",
      " 0.9669\n",
      " 0.9596\n",
      " 0.9929\n",
      " 1.0630\n",
      " 1.0072\n",
      " 0.9891\n",
      " 1.0088\n",
      " 0.9927\n",
      " 0.9813\n",
      " 1.0169\n",
      " 1.0036\n",
      " 0.9925\n",
      " 1.0019\n",
      " 1.0119\n",
      " 0.9890\n",
      " 1.0044\n",
      " 1.0032\n",
      " 1.0093\n",
      " 0.9761\n",
      " 0.9926\n",
      " 0.9940\n",
      " 0.9848\n",
      " 1.0007\n",
      " 0.9982\n",
      " 1.0007\n",
      " 0.9903\n",
      " 1.0256\n",
      " 1.0094\n",
      " 1.0126\n",
      " 0.9809\n",
      " 1.0267\n",
      " 0.9755\n",
      " 0.9612\n",
      " 1.0039\n",
      " 1.0151\n",
      " 0.9983\n",
      " 0.9996\n",
      " 0.9683\n",
      " 0.9946\n",
      " 0.9542\n",
      " 0.9816\n",
      " 0.9823\n",
      " 0.9881\n",
      " 0.9633\n",
      " 0.9903\n",
      " 1.0212\n",
      " 1.0097\n",
      " 0.9887\n",
      " 1.0367\n",
      " 0.9915\n",
      " 0.9775\n",
      " 0.9757\n",
      " 1.0159\n",
      " 0.9996\n",
      " 1.0406\n",
      " 0.9838\n",
      " 1.0173\n",
      " 0.9545\n",
      " 1.0005\n",
      " 0.9975\n",
      " 1.0325\n",
      " 1.0056\n",
      " 0.9829\n",
      " 0.9920\n",
      " 0.9983\n",
      " 0.9770\n",
      " 0.9694\n",
      " 0.9903\n",
      " 0.9887\n",
      " 0.9971\n",
      " 0.9999\n",
      " 1.0030\n",
      " 1.0348\n",
      " 1.0077\n",
      " 1.0157\n",
      " 0.9947\n",
      " 1.0394\n",
      " 1.0082\n",
      " 0.9947\n",
      " 1.0223\n",
      " 0.9548\n",
      " 1.0036\n",
      " 1.0444\n",
      " 1.0071\n",
      " 0.9878\n",
      " 0.9817\n",
      " 0.9491\n",
      " 0.9552\n",
      " 1.0084\n",
      " 0.9830\n",
      "[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 256 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "  3.5273e-02 -6.0187e-04 -8.6909e-03  1.4758e-02\n",
      " -4.6522e-02  2.5530e-03  1.8210e-02  6.4736e-03\n",
      " -2.4215e-02 -1.2809e-02  2.9164e-02 -6.1088e-03\n",
      "  1.4778e-02  2.5837e-03 -1.7173e-02  4.2201e-02\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      " -7.2718e-03 -3.8986e-02 -6.3767e-03 -5.8970e-02\n",
      " -1.8156e-02  6.0860e-03  1.6355e-02  2.1413e-02\n",
      " -8.1049e-03 -5.1852e-03 -3.9745e-02 -4.5802e-03\n",
      "  2.3107e-02 -3.5420e-03 -1.3681e-02 -2.2304e-02\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  1.2137e-03 -1.8335e-02  5.1005e-03  2.3662e-03\n",
      "  9.8527e-03 -5.0290e-02  5.8376e-03  1.2369e-02\n",
      "  3.9473e-02 -2.1408e-02 -1.9980e-03 -1.0648e-02\n",
      " -1.2135e-02 -5.0829e-03  1.9519e-02  2.0986e-03\n",
      "    ... \n",
      "\n",
      "( 0 ,253,.,.) = \n",
      " -1.8667e-02  8.7154e-03  2.6364e-02 -1.1039e-02\n",
      "  3.5996e-02  2.0917e-02  2.1577e-02  1.3649e-02\n",
      " -2.1657e-02  3.0645e-02  2.8878e-02 -2.4981e-02\n",
      " -2.3692e-02  1.0543e-02  7.2246e-04 -3.3984e-02\n",
      "\n",
      "( 0 ,254,.,.) = \n",
      "  2.4917e-02 -1.1054e-02  1.3159e-02 -9.1995e-03\n",
      "  1.6505e-03  1.9821e-03 -1.8875e-02  7.0546e-04\n",
      "  2.9438e-02 -2.5289e-02  2.4973e-02 -8.3844e-03\n",
      " -1.4747e-02  6.7813e-03 -2.1052e-02  2.8421e-03\n",
      "\n",
      "( 0 ,255,.,.) = \n",
      " -2.1643e-02 -2.7994e-03  3.6534e-02  3.6848e-02\n",
      "  1.2547e-02  4.6379e-02 -1.7534e-02  1.2916e-02\n",
      " -1.3008e-02  2.8445e-03 -3.0878e-02  1.3237e-02\n",
      " -4.2453e-02 -3.2827e-02  3.3321e-03 -3.6914e-02\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  3.3678e-02  2.1886e-04 -1.2729e-02 -8.7315e-03\n",
      "  1.9672e-02 -3.3095e-03 -9.7476e-03 -1.9704e-02\n",
      "  2.6309e-03 -4.7498e-02 -2.2374e-02 -2.7507e-02\n",
      " -8.0815e-03  1.2934e-02 -1.9179e-02  4.0134e-03\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  1.5238e-02 -2.7418e-02  2.8855e-02 -4.1971e-03\n",
      " -6.8019e-03  1.3110e-02  5.8033e-04 -2.3897e-02\n",
      " -2.4880e-02  2.2665e-03 -1.6559e-02 -4.4296e-04\n",
      "  3.0181e-02  1.6903e-02 -4.8404e-02 -1.4533e-03\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  3.3703e-02  3.9134e-02 -2.5950e-02  1.0937e-03\n",
      "  5.6794e-03 -3.2152e-03  7.0071e-03 -3.9956e-04\n",
      " -2.7620e-02 -1.0771e-02  4.6144e-03  5.4612e-03\n",
      " -6.5003e-03  1.3064e-02 -7.7552e-03  1.7229e-02\n",
      "    ... \n",
      "\n",
      "( 1 ,253,.,.) = \n",
      "  2.1349e-02  2.3149e-02 -8.5851e-03 -7.8674e-03\n",
      " -9.3104e-03 -1.6675e-02 -2.3800e-03 -2.5962e-02\n",
      " -3.0753e-02  1.9572e-02 -1.0750e-02 -2.9983e-02\n",
      " -1.2610e-02  1.9180e-02  3.7381e-03  1.6164e-03\n",
      "\n",
      "( 1 ,254,.,.) = \n",
      "  4.0004e-02  1.1627e-02  1.7068e-03 -1.8239e-03\n",
      "  1.1895e-02 -2.7433e-04  2.2053e-03 -4.3028e-03\n",
      " -5.9528e-03  1.9076e-03 -2.3870e-02 -3.1842e-02\n",
      "  3.8419e-02  2.4285e-02  1.7469e-03  1.1897e-02\n",
      "\n",
      "( 1 ,255,.,.) = \n",
      "  2.3378e-02  1.0356e-02  1.9196e-02 -5.0178e-03\n",
      "  2.6590e-02 -1.8824e-02 -6.0238e-02  1.8507e-02\n",
      "  6.1359e-03 -1.5353e-05  1.5191e-03  1.8561e-02\n",
      " -4.7854e-03 -6.1432e-03  5.4572e-02  1.7144e-02\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  1.2653e-03 -1.5931e-02  7.1456e-03  5.1644e-03\n",
      " -9.6315e-03 -2.4087e-02  2.9897e-02 -9.9543e-04\n",
      " -4.6163e-04  1.8078e-02  1.2577e-02 -1.8868e-02\n",
      " -2.8557e-02  1.6511e-02 -1.2490e-02 -2.2089e-02\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  2.0232e-02  2.1059e-03  9.8950e-03 -1.9831e-02\n",
      "  1.1962e-02  1.7703e-02  1.9500e-03  2.5214e-02\n",
      "  5.0850e-03  1.3171e-02  5.3366e-03  3.5609e-02\n",
      " -1.2855e-02 -1.7627e-02 -1.9889e-02  1.2816e-02\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      " -1.8577e-02 -3.9746e-03  2.8467e-02  3.5260e-04\n",
      "  3.3356e-03  4.2140e-03 -4.8251e-02 -9.8047e-03\n",
      " -1.9562e-02  1.5247e-02 -2.7409e-02 -1.0281e-02\n",
      "  1.5635e-02  7.3995e-03 -3.5173e-02  4.3348e-02\n",
      "    ... \n",
      "\n",
      "( 2 ,253,.,.) = \n",
      "  2.8719e-02  1.1490e-03 -1.0804e-02 -7.3737e-03\n",
      " -1.1368e-03  2.4120e-02  2.5473e-03  1.5478e-02\n",
      " -3.5005e-03  2.5003e-02 -1.2345e-02 -1.6613e-02\n",
      "  2.3815e-02  9.1280e-04  7.6151e-03  9.3566e-03\n",
      "\n",
      "( 2 ,254,.,.) = \n",
      " -1.6367e-02  2.0448e-02  4.5894e-02  3.8355e-02\n",
      "  1.2712e-02 -2.2640e-02  3.0615e-02  2.9447e-02\n",
      " -1.4467e-02  4.2981e-02 -3.3761e-02  3.1991e-03\n",
      "  2.1351e-02 -2.6958e-02  7.3740e-03  8.7761e-03\n",
      "\n",
      "( 2 ,255,.,.) = \n",
      " -3.8690e-02  5.1809e-04 -8.4389e-03  2.8561e-04\n",
      "  1.6397e-02  1.8988e-02  4.1453e-03  1.0854e-02\n",
      "  7.9894e-03  4.1051e-03  6.0201e-03  1.1077e-02\n",
      " -1.9288e-02 -1.9061e-03 -2.8733e-02  7.8256e-03\n",
      "...     \n",
      "      ⋮  \n",
      "\n",
      "(509, 0 ,.,.) = \n",
      " -9.0926e-03 -3.8467e-02 -2.1494e-02 -1.9764e-02\n",
      " -2.9933e-02  1.4185e-02 -8.3215e-04 -2.9654e-02\n",
      "  9.7213e-04 -1.0118e-02 -1.2354e-02 -3.6201e-02\n",
      " -1.4284e-02 -1.2374e-02 -1.5260e-02 -1.9501e-02\n",
      "\n",
      "(509, 1 ,.,.) = \n",
      " -7.8958e-03  2.2302e-02 -1.9562e-02  7.0357e-03\n",
      " -7.3376e-04  2.2413e-02 -4.0719e-02  1.5877e-02\n",
      "  1.6567e-03 -1.0952e-02 -4.2733e-03 -2.1319e-02\n",
      " -3.7912e-02 -2.3083e-02  6.3195e-02 -1.3388e-02\n",
      "\n",
      "(509, 2 ,.,.) = \n",
      "  2.4945e-02  8.3224e-03 -5.6253e-03  9.2145e-03\n",
      " -1.4153e-04  7.8093e-03  9.3675e-03 -4.6700e-02\n",
      "  3.2583e-03  1.2680e-02  1.2055e-02 -1.7596e-02\n",
      " -2.1093e-02 -1.5240e-02  2.0044e-02 -3.3351e-02\n",
      "    ... \n",
      "\n",
      "(509,253,.,.) = \n",
      "  1.9818e-02  4.2252e-02 -4.9483e-03  2.2087e-02\n",
      " -8.0841e-04 -1.9864e-02  3.1444e-02 -1.0839e-02\n",
      " -1.1763e-02 -4.9056e-02 -1.3985e-02 -8.0749e-04\n",
      " -2.8506e-02 -4.1079e-02 -4.0910e-02  7.7752e-03\n",
      "\n",
      "(509,254,.,.) = \n",
      " -2.2102e-02  1.1763e-02 -4.3967e-03 -2.3138e-03\n",
      "  3.4375e-03 -1.4381e-02  4.2943e-03  3.4192e-02\n",
      " -1.4632e-02  2.7429e-02  1.4671e-03 -6.3985e-03\n",
      " -3.1573e-02 -9.6609e-03 -3.0607e-03 -5.0091e-03\n",
      "\n",
      "(509,255,.,.) = \n",
      " -2.7410e-02  3.4372e-02  1.9198e-02 -1.2448e-02\n",
      "  2.6008e-02  2.4062e-02 -5.8430e-03 -1.2820e-02\n",
      " -1.5839e-02  2.6165e-02  1.2775e-02  2.5743e-02\n",
      "  1.6093e-02  5.2876e-03  7.8091e-03  2.7503e-02\n",
      "      ⋮  \n",
      "\n",
      "(510, 0 ,.,.) = \n",
      " -7.5698e-03 -1.9220e-02 -1.9287e-02 -1.2303e-02\n",
      "  1.1896e-02  2.0921e-03 -5.2834e-03 -2.0545e-02\n",
      "  1.2851e-02  9.3462e-03 -8.7643e-03  4.9267e-03\n",
      "  1.4887e-02 -3.3785e-02  2.8916e-03 -7.4187e-03\n",
      "\n",
      "(510, 1 ,.,.) = \n",
      "  3.1092e-02 -2.3534e-03  1.0819e-02  1.4733e-02\n",
      " -7.9760e-03  1.2845e-02 -1.5228e-03  7.9843e-03\n",
      " -4.5315e-02 -2.5218e-03  2.6354e-02  2.3180e-02\n",
      " -1.3764e-02 -2.9207e-02  2.7692e-02 -1.9163e-02\n",
      "\n",
      "(510, 2 ,.,.) = \n",
      "  1.9290e-02 -2.6892e-04  4.5815e-03 -1.5909e-03\n",
      " -1.7486e-02 -1.2263e-02 -3.4514e-02  9.3251e-03\n",
      " -1.0792e-02  2.5750e-02  1.0402e-03  1.4775e-02\n",
      " -2.7819e-02  3.1450e-03 -4.0714e-02 -4.0423e-02\n",
      "    ... \n",
      "\n",
      "(510,253,.,.) = \n",
      "  6.9595e-03 -9.0311e-03 -1.9653e-02 -1.0793e-02\n",
      " -1.3875e-02 -3.3115e-02  3.3428e-02 -1.8220e-02\n",
      "  2.5925e-03 -1.7207e-02  1.1843e-02 -2.0025e-03\n",
      " -1.5799e-02 -2.2416e-02 -3.4744e-02  4.4905e-02\n",
      "\n",
      "(510,254,.,.) = \n",
      " -1.7770e-02  2.9550e-02  5.3744e-03 -2.2926e-02\n",
      "  2.1547e-02  3.5001e-02  4.9501e-03  2.1247e-02\n",
      " -5.3341e-02  1.8125e-02  1.8499e-02 -3.0846e-03\n",
      " -4.8948e-02 -1.7342e-02 -8.5234e-03  2.0215e-02\n",
      "\n",
      "(510,255,.,.) = \n",
      "  1.1877e-02 -5.3987e-02 -2.4494e-02 -1.5357e-02\n",
      "  2.7809e-02 -1.0676e-02 -1.0944e-02  3.2888e-02\n",
      " -7.8817e-03  7.2952e-03 -1.1587e-03  1.3063e-02\n",
      "  3.5485e-03 -2.6642e-02  1.8210e-04 -1.6427e-02\n",
      "      ⋮  \n",
      "\n",
      "(511, 0 ,.,.) = \n",
      "  4.0393e-03 -2.2472e-02  2.0230e-02  1.8364e-02\n",
      " -1.3651e-02 -1.0357e-02  3.0096e-02  9.2919e-03\n",
      " -5.4164e-03  7.0098e-03 -1.1823e-03 -1.5506e-03\n",
      "  8.0593e-03  2.6641e-03 -2.5252e-02 -9.3334e-03\n",
      "\n",
      "(511, 1 ,.,.) = \n",
      " -1.6125e-02 -1.4311e-02  4.9319e-02  1.7638e-03\n",
      "  2.5168e-02  9.1449e-03  1.3712e-02 -1.0164e-02\n",
      "  2.0945e-02 -1.6954e-02  4.1673e-02  1.0498e-02\n",
      "  2.1157e-02 -1.0311e-03  1.2532e-02 -1.7738e-02\n",
      "\n",
      "(511, 2 ,.,.) = \n",
      "  1.7586e-02  1.0764e-02  1.4340e-03  9.0986e-03\n",
      " -2.4437e-02 -6.1968e-04 -5.0962e-04 -2.6294e-03\n",
      "  3.7234e-02  3.2866e-02  3.8989e-03  4.5239e-02\n",
      "  2.1149e-02  2.7160e-02  3.2817e-03 -1.3291e-02\n",
      "    ... \n",
      "\n",
      "(511,253,.,.) = \n",
      " -4.2726e-03 -1.8286e-02 -1.2675e-03  7.5580e-03\n",
      "  3.5826e-02 -1.6190e-02 -1.9283e-02  2.6095e-02\n",
      "  1.2710e-03  9.1361e-03 -3.7854e-02  3.0959e-02\n",
      " -3.1469e-02  3.8534e-02 -7.9464e-03 -6.2563e-02\n",
      "\n",
      "(511,254,.,.) = \n",
      "  4.5584e-02  1.0392e-02 -6.9698e-03 -1.2438e-02\n",
      "  1.9397e-02  8.8095e-03 -1.2929e-02  1.6326e-03\n",
      "  6.1820e-02  1.4227e-02 -2.5793e-03 -2.7982e-02\n",
      " -8.2551e-03 -2.2906e-02  1.5833e-02 -2.9930e-02\n",
      "\n",
      "(511,255,.,.) = \n",
      " -1.3221e-02  5.3507e-03  3.7892e-03 -2.9326e-02\n",
      " -1.9702e-03  3.1320e-02 -7.0919e-03  2.3615e-03\n",
      " -9.7470e-03  1.3265e-02 -2.6192e-02  1.1356e-02\n",
      " -2.5695e-02  1.3609e-02 -1.2737e-02  7.7721e-03\n",
      "[torch.cuda.FloatTensor of size 512x256x4x4 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0.9988\n",
      " 1.0135\n",
      " 1.0207\n",
      " 1.0138\n",
      " 0.9882\n",
      " 0.9917\n",
      " 1.0154\n",
      " 1.0095\n",
      " 0.9854\n",
      " 0.9868\n",
      " 1.0351\n",
      " 1.0094\n",
      " 0.9978\n",
      " 1.0433\n",
      " 1.0284\n",
      " 1.0069\n",
      " 1.0237\n",
      " 0.9955\n",
      " 1.0072\n",
      " 0.9907\n",
      " 0.9879\n",
      " 0.9985\n",
      " 0.9831\n",
      " 0.9666\n",
      " 0.9781\n",
      " 0.9757\n",
      " 1.0310\n",
      " 0.9977\n",
      " 0.9971\n",
      " 0.9667\n",
      " 1.0408\n",
      " 1.0287\n",
      " 1.0182\n",
      " 1.0097\n",
      " 0.9877\n",
      " 0.9611\n",
      " 1.0304\n",
      " 0.9936\n",
      " 0.9867\n",
      " 0.9921\n",
      " 1.0170\n",
      " 0.9865\n",
      " 1.0027\n",
      " 0.9962\n",
      " 0.9990\n",
      " 0.9772\n",
      " 1.0289\n",
      " 1.0078\n",
      " 0.9998\n",
      " 0.9636\n",
      " 1.0225\n",
      " 0.9955\n",
      " 1.0066\n",
      " 0.9725\n",
      " 0.9432\n",
      " 1.0081\n",
      " 1.0027\n",
      " 0.9792\n",
      " 0.9935\n",
      " 1.0117\n",
      " 1.0122\n",
      " 0.9918\n",
      " 0.9653\n",
      " 0.9887\n",
      " 0.9615\n",
      " 1.0131\n",
      " 0.9971\n",
      " 0.9800\n",
      " 1.0187\n",
      " 0.9792\n",
      " 1.0365\n",
      " 0.9823\n",
      " 1.0242\n",
      " 1.0054\n",
      " 0.9691\n",
      " 1.0121\n",
      " 1.0099\n",
      " 0.9645\n",
      " 0.9538\n",
      " 0.9914\n",
      " 0.9991\n",
      " 0.9908\n",
      " 0.9795\n",
      " 0.9820\n",
      " 0.9917\n",
      " 1.0075\n",
      " 0.9968\n",
      " 1.0008\n",
      " 0.9778\n",
      " 1.0075\n",
      " 1.0200\n",
      " 0.9859\n",
      " 0.9997\n",
      " 0.9882\n",
      " 0.9968\n",
      " 1.0078\n",
      " 1.0088\n",
      " 0.9842\n",
      " 0.9811\n",
      " 0.9957\n",
      " 1.0270\n",
      " 0.9843\n",
      " 1.0014\n",
      " 0.9915\n",
      " 0.9632\n",
      " 0.9893\n",
      " 1.0214\n",
      " 0.9960\n",
      " 0.9543\n",
      " 1.0233\n",
      " 0.9642\n",
      " 1.0095\n",
      " 0.9956\n",
      " 0.9893\n",
      " 0.9852\n",
      " 1.0038\n",
      " 1.0112\n",
      " 1.0088\n",
      " 1.0160\n",
      " 0.9720\n",
      " 1.0070\n",
      " 1.0052\n",
      " 1.0091\n",
      " 1.0036\n",
      " 1.0138\n",
      " 1.0126\n",
      " 0.9849\n",
      " 0.9986\n",
      " 0.9652\n",
      " 0.9882\n",
      " 1.0096\n",
      " 1.0012\n",
      " 1.0051\n",
      " 0.9711\n",
      " 1.0011\n",
      " 0.9903\n",
      " 1.0084\n",
      " 1.0088\n",
      " 0.9856\n",
      " 0.9914\n",
      " 0.9906\n",
      " 1.0279\n",
      " 1.0133\n",
      " 0.9929\n",
      " 1.0159\n",
      " 1.0337\n",
      " 0.9936\n",
      " 0.9905\n",
      " 1.0115\n",
      " 1.0046\n",
      " 0.9996\n",
      " 1.0119\n",
      " 0.9817\n",
      " 0.9799\n",
      " 0.9790\n",
      " 1.0101\n",
      " 0.9916\n",
      " 1.0001\n",
      " 0.9781\n",
      " 0.9841\n",
      " 0.9891\n",
      " 0.9758\n",
      " 1.0122\n",
      " 0.9910\n",
      " 0.9926\n",
      " 0.9996\n",
      " 0.9827\n",
      " 1.0121\n",
      " 0.9851\n",
      " 0.9876\n",
      " 0.9963\n",
      " 0.9924\n",
      " 0.9512\n",
      " 0.9945\n",
      " 1.0083\n",
      " 1.0121\n",
      " 1.0161\n",
      " 0.9708\n",
      " 0.9918\n",
      " 1.0132\n",
      " 1.0226\n",
      " 1.0081\n",
      " 1.0407\n",
      " 1.0309\n",
      " 0.9784\n",
      " 1.0297\n",
      " 1.0134\n",
      " 1.0074\n",
      " 1.0100\n",
      " 0.9853\n",
      " 1.0054\n",
      " 0.9866\n",
      " 0.9856\n",
      " 0.9949\n",
      " 1.0189\n",
      " 1.0219\n",
      " 1.0364\n",
      " 1.0143\n",
      " 1.0185\n",
      " 0.9915\n",
      " 1.0141\n",
      " 1.0143\n",
      " 1.0095\n",
      " 0.9662\n",
      " 1.0287\n",
      " 0.9949\n",
      " 1.0182\n",
      " 0.9924\n",
      " 1.0078\n",
      " 0.9456\n",
      " 1.0298\n",
      " 0.9986\n",
      " 0.9997\n",
      " 0.9867\n",
      " 1.0018\n",
      " 0.9947\n",
      " 0.9836\n",
      " 1.0066\n",
      " 0.9723\n",
      " 0.9806\n",
      " 1.0225\n",
      " 0.9806\n",
      " 0.9421\n",
      " 1.0415\n",
      " 1.0035\n",
      " 1.0175\n",
      " 0.9980\n",
      " 0.9802\n",
      " 1.0097\n",
      " 0.9961\n",
      " 0.9884\n",
      " 0.9933\n",
      " 0.9747\n",
      " 0.9899\n",
      " 0.9998\n",
      " 1.0230\n",
      " 0.9900\n",
      " 0.9783\n",
      " 1.0077\n",
      " 1.0034\n",
      " 1.0239\n",
      " 0.9985\n",
      " 1.0116\n",
      " 0.9896\n",
      " 0.9965\n",
      " 1.0105\n",
      " 0.9682\n",
      " 1.0024\n",
      " 1.0004\n",
      " 0.9652\n",
      " 0.9759\n",
      " 1.0029\n",
      " 1.0066\n",
      " 0.9772\n",
      " 0.9876\n",
      " 1.0252\n",
      " 0.9733\n",
      " 0.9870\n",
      " 1.0245\n",
      " 0.9690\n",
      " 1.0109\n",
      " 0.9576\n",
      " 1.0054\n",
      " 0.9837\n",
      " 1.0414\n",
      " 1.0147\n",
      " 0.9819\n",
      " 1.0109\n",
      " 1.0013\n",
      " 0.9932\n",
      " 0.9885\n",
      " 1.0390\n",
      " 1.0247\n",
      " 1.0234\n",
      " 1.0373\n",
      " 1.0280\n",
      " 1.0065\n",
      " 1.0072\n",
      " 0.9641\n",
      " 1.0339\n",
      " 1.0167\n",
      " 1.0101\n",
      " 1.0400\n",
      " 0.9651\n",
      " 1.0386\n",
      " 1.0087\n",
      " 1.0187\n",
      " 0.9676\n",
      " 1.0194\n",
      " 0.9751\n",
      " 0.9984\n",
      " 0.9805\n",
      " 1.0189\n",
      " 1.0089\n",
      " 1.0021\n",
      " 1.0024\n",
      " 0.9874\n",
      " 1.0041\n",
      " 0.9993\n",
      " 1.0305\n",
      " 0.9640\n",
      " 0.9970\n",
      " 1.0321\n",
      " 0.9579\n",
      " 1.0623\n",
      " 0.9890\n",
      " 0.9994\n",
      " 1.0464\n",
      " 1.0309\n",
      " 1.0451\n",
      " 1.0089\n",
      " 1.0072\n",
      " 0.9752\n",
      " 0.9913\n",
      " 0.9864\n",
      " 1.0001\n",
      " 0.9745\n",
      " 0.9840\n",
      " 1.0081\n",
      " 1.0087\n",
      " 0.9759\n",
      " 1.0017\n",
      " 0.9765\n",
      " 0.9749\n",
      " 1.0040\n",
      " 0.9849\n",
      " 1.0097\n",
      " 0.9632\n",
      " 1.0164\n",
      " 0.9806\n",
      " 0.9852\n",
      " 0.9715\n",
      " 0.9851\n",
      " 0.9995\n",
      " 0.9737\n",
      " 1.0019\n",
      " 1.0024\n",
      " 0.9929\n",
      " 1.0393\n",
      " 0.9987\n",
      " 1.0222\n",
      " 0.9847\n",
      " 1.0027\n",
      " 0.9962\n",
      " 1.0086\n",
      " 1.0004\n",
      " 1.0259\n",
      " 0.9956\n",
      " 0.9861\n",
      " 0.9593\n",
      " 0.9780\n",
      " 0.9989\n",
      " 1.0060\n",
      " 0.9738\n",
      " 1.0076\n",
      " 0.9906\n",
      " 0.9861\n",
      " 0.9839\n",
      " 0.9987\n",
      " 0.9931\n",
      " 1.0492\n",
      " 0.9880\n",
      " 0.9748\n",
      " 0.9888\n",
      " 0.9937\n",
      " 1.0019\n",
      " 1.0078\n",
      " 0.9857\n",
      " 0.9864\n",
      " 1.0165\n",
      " 1.0178\n",
      " 0.9917\n",
      " 1.0158\n",
      " 1.0248\n",
      " 1.0156\n",
      " 1.0006\n",
      " 1.0286\n",
      " 1.0115\n",
      " 0.9864\n",
      " 1.0205\n",
      " 0.9811\n",
      " 0.9817\n",
      " 1.0219\n",
      " 1.0209\n",
      " 1.0239\n",
      " 1.0066\n",
      " 0.9838\n",
      " 0.9886\n",
      " 1.0206\n",
      " 0.9766\n",
      " 1.0074\n",
      " 0.9682\n",
      " 0.9974\n",
      " 0.9821\n",
      " 1.0175\n",
      " 0.9840\n",
      " 0.9813\n",
      " 1.0078\n",
      " 0.9863\n",
      " 0.9896\n",
      " 0.9910\n",
      " 1.0202\n",
      " 0.9938\n",
      " 1.0033\n",
      " 1.0132\n",
      " 1.0214\n",
      " 0.9592\n",
      " 1.0095\n",
      " 0.9791\n",
      " 0.9980\n",
      " 0.9957\n",
      " 1.0258\n",
      " 0.9963\n",
      " 1.0274\n",
      " 0.9971\n",
      " 1.0178\n",
      " 0.9791\n",
      " 1.0168\n",
      " 1.0030\n",
      " 0.9729\n",
      " 0.9517\n",
      " 1.0000\n",
      " 1.0207\n",
      " 0.9787\n",
      " 0.9890\n",
      " 0.9907\n",
      " 1.0261\n",
      " 1.0059\n",
      " 0.9988\n",
      " 0.9936\n",
      " 1.0183\n",
      " 1.0099\n",
      " 1.0195\n",
      " 0.9917\n",
      " 1.0068\n",
      " 0.9664\n",
      " 0.9853\n",
      " 1.0188\n",
      " 1.0013\n",
      " 0.9793\n",
      " 1.0194\n",
      " 0.9985\n",
      " 1.0233\n",
      " 1.0169\n",
      " 0.9788\n",
      " 1.0061\n",
      " 0.9879\n",
      " 0.9831\n",
      " 0.9965\n",
      " 1.0035\n",
      " 0.9884\n",
      " 1.0079\n",
      " 0.9898\n",
      " 0.9686\n",
      " 0.9926\n",
      " 0.9946\n",
      " 1.0048\n",
      " 0.9599\n",
      " 1.0014\n",
      " 1.0191\n",
      " 1.0267\n",
      " 0.9878\n",
      " 1.0288\n",
      " 1.0253\n",
      " 1.0150\n",
      " 1.0182\n",
      " 0.9842\n",
      " 0.9868\n",
      " 0.9809\n",
      " 1.0044\n",
      " 1.0190\n",
      " 0.9957\n",
      " 0.9663\n",
      " 1.0016\n",
      " 1.0100\n",
      " 1.0080\n",
      " 1.0302\n",
      " 1.0086\n",
      " 1.0041\n",
      " 1.0195\n",
      " 0.9972\n",
      " 0.9569\n",
      " 1.0172\n",
      " 1.0209\n",
      " 0.9977\n",
      " 0.9863\n",
      " 1.0347\n",
      " 0.9958\n",
      " 1.0053\n",
      " 1.0025\n",
      " 1.0066\n",
      " 1.0136\n",
      " 1.0020\n",
      " 0.9884\n",
      " 0.9958\n",
      " 0.9860\n",
      " 0.9744\n",
      " 0.9554\n",
      " 0.9905\n",
      " 1.0137\n",
      " 0.9732\n",
      " 0.9795\n",
      " 1.0254\n",
      " 0.9876\n",
      " 1.0067\n",
      " 0.9758\n",
      " 0.9963\n",
      " 0.9683\n",
      " 1.0225\n",
      " 1.0183\n",
      " 0.9595\n",
      " 1.0210\n",
      "[torch.cuda.FloatTensor of size 512 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 512 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "-8.7143e-03  6.7719e-03  4.9627e-03  ...  -1.2873e-02 -1.2371e-02 -1.1028e-02\n",
      " 1.0859e-02  1.4421e-02 -4.5889e-02  ...   3.7528e-03 -8.4551e-03 -1.9646e-02\n",
      " 7.6990e-04  2.2058e-02  1.7924e-02  ...   2.7274e-02 -1.9230e-02 -6.0116e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-1.2643e-02 -4.6825e-04 -8.2285e-03  ...  -1.2169e-02  1.4340e-02 -1.1174e-02\n",
      " 5.6514e-03 -1.3886e-02  1.6828e-02  ...   4.5509e-03  1.8638e-02  1.0388e-02\n",
      "-1.8693e-02  6.3943e-03 -4.3107e-03  ...   1.3471e-02  7.2525e-03 -1.8134e-04\n",
      "[torch.cuda.FloatTensor of size 2048x8192 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "⋮ \n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 2048 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 9.4984e-04  4.0839e-02 -7.7676e-03  ...   3.9720e-03  8.2324e-03 -3.1111e-02\n",
      " 5.6605e-02 -1.4423e-02 -9.0164e-03  ...   4.6860e-02  2.8277e-03 -4.0933e-02\n",
      " 4.9448e-02  4.1375e-03  8.9488e-03  ...  -1.2581e-02  1.4797e-02 -1.7033e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-4.8907e-03 -2.1853e-02  8.7454e-04  ...  -4.5948e-02 -1.7190e-02  6.2775e-03\n",
      "-3.4579e-02 -1.1591e-02  1.9876e-02  ...   9.7335e-03 -1.1066e-02 -5.7795e-03\n",
      "-3.2157e-03 -1.5883e-03 -1.0373e-02  ...   3.6208e-03  2.7621e-02  3.0786e-02\n",
      "[torch.cuda.FloatTensor of size 2048x2048 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "⋮ \n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 2048 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "-0.0318 -0.0138 -0.0151  ...  -0.0420 -0.0391 -0.0036\n",
      "-0.0423 -0.0019  0.0093  ...  -0.0052  0.0185 -0.0238\n",
      "[torch.cuda.FloatTensor of size 2x2048 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      "[torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ii in list(model.parameters()):\n",
    "    print ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r=list(model.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr=list(r.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rr.modules())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       " 0.1884 -0.1491\n",
       "[torch.cuda.FloatTensor of size 256x2 (GPU 0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 4, 4])\n",
      "torch.Size([128, 64, 4, 4])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 4, 4])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256, 4, 4])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([2048, 8192])\n",
      "torch.Size([2048])\n",
      "torch.Size([2048, 2048])\n",
      "torch.Size([2048])\n",
      "torch.Size([2, 2048])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for ii in model.parameters():\n",
    "    print ii.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm=model._modules.get('features')._modules['model']._modules['bnorm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "    ... \n",
       "\n",
       "( 0 ,125,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 0 ,126,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 0 ,127,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "      ⋮  \n",
       "\n",
       "( 1 , 0 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 1 , 1 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 1 , 2 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "    ... \n",
       "\n",
       "( 1 ,125,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 1 ,126,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 1 ,127,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "      ⋮  \n",
       "\n",
       "( 2 , 0 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 2 , 1 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 2 , 2 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "    ... \n",
       "\n",
       "( 2 ,125,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 2 ,126,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "( 2 ,127,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "...     \n",
       "      ⋮  \n",
       "\n",
       "(125, 0 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(125, 1 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(125, 2 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "    ... \n",
       "\n",
       "(125,125,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(125,126,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(125,127,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "      ⋮  \n",
       "\n",
       "(126, 0 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(126, 1 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(126, 2 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "    ... \n",
       "\n",
       "(126,125,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(126,126,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(126,127,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "      ⋮  \n",
       "\n",
       "(127, 0 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(127, 1 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(127, 2 ,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "    ... \n",
       "\n",
       "(127,125,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(127,126,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "\n",
       "(127,127,.,.) = \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "     ...       ⋱       ...    \n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       " nan nan nan  ...  nan nan nan\n",
       "[torch.cuda.FloatTensor of size 128x128x16x16 (GPU 0)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(Variable(t.ones(128,128,16,16)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', \n",
       "               1.0116\n",
       "               1.0103\n",
       "               1.0261\n",
       "               1.0188\n",
       "               1.0030\n",
       "               1.0272\n",
       "               1.0045\n",
       "               1.0113\n",
       "               1.0468\n",
       "               1.0065\n",
       "               1.0120\n",
       "               1.0227\n",
       "               1.0256\n",
       "               1.0049\n",
       "               0.9773\n",
       "               1.0045\n",
       "               1.0165\n",
       "               1.0153\n",
       "               0.9943\n",
       "               1.0054\n",
       "               0.9903\n",
       "               0.9961\n",
       "               1.0029\n",
       "               1.0181\n",
       "               1.0102\n",
       "               0.9919\n",
       "               1.0241\n",
       "               1.0282\n",
       "               1.0351\n",
       "               0.9941\n",
       "               1.0222\n",
       "               1.0032\n",
       "               0.9999\n",
       "               1.0166\n",
       "               1.0179\n",
       "               0.9954\n",
       "               1.0020\n",
       "               1.0060\n",
       "               0.9920\n",
       "               0.9797\n",
       "               0.9986\n",
       "               1.0179\n",
       "               0.9915\n",
       "               1.0130\n",
       "               0.9945\n",
       "               0.9834\n",
       "               1.0084\n",
       "               1.0217\n",
       "               0.9730\n",
       "               1.0062\n",
       "               0.9825\n",
       "               1.0071\n",
       "               1.0105\n",
       "               1.0082\n",
       "               1.0145\n",
       "               1.0206\n",
       "               1.0204\n",
       "               1.0131\n",
       "               1.0297\n",
       "               0.9788\n",
       "               0.9856\n",
       "               0.9827\n",
       "               0.9786\n",
       "               0.9805\n",
       "               0.9805\n",
       "               0.9762\n",
       "               1.0250\n",
       "               1.0184\n",
       "               0.9824\n",
       "               1.0384\n",
       "               1.0147\n",
       "               0.9857\n",
       "               0.9941\n",
       "               1.0025\n",
       "               0.9936\n",
       "               0.9789\n",
       "               0.9902\n",
       "               1.0474\n",
       "               1.0156\n",
       "               1.0206\n",
       "               1.0002\n",
       "               0.9734\n",
       "               1.0273\n",
       "               1.0072\n",
       "               1.0020\n",
       "               0.9829\n",
       "               1.0216\n",
       "               1.0329\n",
       "               0.9813\n",
       "               1.0163\n",
       "               0.9864\n",
       "               0.9925\n",
       "               1.0099\n",
       "               0.9920\n",
       "               1.0242\n",
       "               1.0103\n",
       "               0.9739\n",
       "               0.9937\n",
       "               1.0235\n",
       "               1.0219\n",
       "               1.0056\n",
       "               1.0065\n",
       "               1.0205\n",
       "               0.9798\n",
       "               0.9915\n",
       "               1.0262\n",
       "               0.9892\n",
       "               0.9921\n",
       "               0.9683\n",
       "               0.9995\n",
       "               1.0017\n",
       "               0.9895\n",
       "               0.9603\n",
       "               0.9837\n",
       "               0.9903\n",
       "               1.0096\n",
       "               0.9865\n",
       "               1.0126\n",
       "               1.0249\n",
       "               0.9982\n",
       "               1.0048\n",
       "               1.0204\n",
       "               0.9917\n",
       "               0.9826\n",
       "               1.0189\n",
       "               0.9835\n",
       "               0.9941\n",
       "               1.0196\n",
       "              [torch.cuda.FloatTensor of size 128 (GPU 0)]), ('bias', \n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "               0\n",
       "              [torch.cuda.FloatTensor of size 128 (GPU 0)]), ('running_mean', \n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              [torch.cuda.FloatTensor of size 128 (GPU 0)]), ('running_var', \n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              nan\n",
       "              [torch.cuda.FloatTensor of size 128 (GPU 0)])])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
